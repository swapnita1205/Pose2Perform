{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "id": "D5CmW5MlsjcQ"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install -U yt_dlp youtube-search-python"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install moviepy"
      ],
      "metadata": {
        "id": "UhyOjiXZBKvK"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install \"httpx<0.27\" --force-reinstall"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Y4x747NJs27a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!python3 -m pip install opencv-python==4.9.0.80 mediapipe==0.10.5 torch==2.2.0"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dbtiKV52wn41"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from youtubesearchpython import VideosSearch\n",
        "import yt_dlp\n",
        "import os\n",
        "\n",
        "def parse_duration(duration_str):\n",
        "    parts = duration_str.split(':')\n",
        "    if len(parts) == 2:  # mm:ss\n",
        "        minutes, seconds = map(int, parts)\n",
        "        return minutes * 60 + seconds\n",
        "    elif len(parts) == 3:  # hh:mm:ss\n",
        "        hours, minutes, seconds = map(int, parts)\n",
        "        return hours * 3600 + minutes * 60 + seconds\n",
        "    return 0  # if unknown or invalid\n",
        "\n",
        "def download_videos(query, label, num_videos=15, save_dir='videos'):\n",
        "    path = os.path.join(save_dir, label)\n",
        "    os.makedirs(path, exist_ok=True)\n",
        "\n",
        "    collected = 0\n",
        "    search = VideosSearch(query, limit=30)  # Fetch more to filter\n",
        "\n",
        "    for result in search.result()['result']:\n",
        "        if 'duration' not in result:\n",
        "            continue  # Skip livestreams or missing info\n",
        "\n",
        "        duration_sec = parse_duration(result['duration'])\n",
        "        if duration_sec >= 300:\n",
        "            continue  # Skip videos 5 min or longer\n",
        "\n",
        "        url = result['link']\n",
        "        output_filename = os.path.join(path, f\"{label}_{collected + 1}.mp4\")\n",
        "\n",
        "        ydl_opts = {\n",
        "            'format': 'best[ext=mp4]/best',\n",
        "            'outtmpl': output_filename,\n",
        "            'quiet': True,\n",
        "            'noplaylist': True,\n",
        "        }\n",
        "\n",
        "        try:\n",
        "            print(f\"Downloading [{label}] video {collected + 1}: {result['title']}\")\n",
        "            with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "                ydl.download([url])\n",
        "            collected += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to download {url}: {e}\")\n",
        "\n",
        "        if collected >= num_videos:\n",
        "            break\n",
        "\n",
        "# Download 15 short ballet and 15 short hip-hop videos\n",
        "download_videos(\"ballet dance performance\", \"ballet\", num_videos=15)\n",
        "download_videos(\"hip hop dance performance\", \"hiphop\", num_videos=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "waUTScNZskzz",
        "outputId": "69745be6-60cf-4713-b143-f203843a84e8",
        "collapsed": true
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading [ballet] video 1: Jeeho Lee WOWS the audience with the La Esmeralda Finale!\n",
            "Downloading [ballet] video 2: Dance of the Sugar Plum Fairy from The Nutcracker (The Royal Ballet)\n",
            "Downloading [ballet] video 3: Swan Lake ‚Äì Dance of the cygnets (The Royal Ballet)\n",
            "Downloading [ballet] video 4: Ella is FLYING üòçü©∞‚ú® #ballerina #ballet #shorts #ad\n",
            "Downloading [ballet] video 5: OKC Ballet collaborates with Dance for Parkinson's group for unique performance\n",
            "Downloading [ballet] video 6: BALLET in 30 sec - GISELLE - Maria Khoreva in #shorts\n",
            "Downloading [ballet] video 7: WOAH!! THAT BALANCE üò± @tessa_rivadulla #ballet #balletclass\n",
            "Downloading [ballet] video 8: Ballet is beautiful ü•π‚ù§Ô∏èü©∞ #ballet #shorts #shortfilm\n",
            "Downloading [ballet] video 9: Watch Our Favorite Tiny Dancer Perform the Nutcracker with the New York City Ballet\n",
            "Downloading [ballet] video 10: Don Quixote ‚Äì Act III Kitri Variation (Akane Takada, The Royal Ballet)\n",
            "Downloading [ballet] video 11: Swan Lake ‚Äì End of Act II (The Royal Ballet) #shorts #RoyalOperaHouse\n",
            "Downloading [ballet] video 12: Audrey‚Äôs turns ‚ú®ü©∞ #ballerina #ballet #balletdancer #balletworld #pointe\n",
            "Downloading [ballet] video 13: Swan's Ballet | Dance Along | Pinkfong Songs for Children\n",
            "Downloading [hiphop] video 1: Louisiana State University Dance Team Hip Hop 2022\n",
            "Downloading [hiphop] video 2: THE ROYAL FAMILY - Nationals 2018 (Guest Performance)\n",
            "Downloading [hiphop] video 3: Int/Snr Hip Hop Troupe - Legends\n",
            "Downloading [hiphop] video 4: The knee drop was crazy üò± #dance\n",
            "Downloading [hiphop] video 5: 2DAY SQUAD | 2023 SHOWCASE\n",
            "Downloading [hiphop] video 6: Teen Hip Hop- Best of Rihanna\n",
            "Downloading [hiphop] video 7: Mini Hip Hop- ‚ÄúThee Mini Stallions‚Äù (Dance Nation)\n",
            "Downloading [hiphop] video 8: Malaika Arora Ko Kisne Kiya Impress? üòØ | Realme Hip Hop India S2 | Amazon MX Player\n",
            "Downloading [hiphop] video 9: Krump King Big Mijo Aur Hectic The Wildest Dance Battle| Hip Hop India season 2| Dance Reality Show\n",
            "Downloading [hiphop] video 10: Malaika Arora Ko Kisne Kiya Impress? üòØ | Realme Hip Hop India S2 | Amazon MX Player\n",
            "Downloading [hiphop] video 11: Pal Pal Song Dance Video ‚ù§Ô∏è‚ú® @afusic #hiphop #dance #hiphopdance\n",
            "Downloading [hiphop] video 12: Raghav Juyal And Shakti Mohan Ka Reunion? | Realme Hip Hop India S2 | Amazon MX Player #dance\n",
            "Downloading [hiphop] video 13: ON THE FLOOR - Jennifer Lopez ft Pitbull | Trang Ex Dance Fitness\n",
            "Downloading [hiphop] video 14: \"Girls Night\" Teen Hip-Hop Large Group Dance\n",
            "Downloading [hiphop] video 15: COS Dance Team - Gasolina Hip Hop Dance\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.26.4 --force-reinstall"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7gOJET0q9m0",
        "outputId": "9a0254d3-ae2d-412e-aca9-cedd89cb09f0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/61.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from moviepy.editor import VideoFileClip\n",
        "import os\n",
        "\n",
        "def split_video(video_path, clip_length=30, output_dir=\"clips\"):\n",
        "    try:\n",
        "        video = VideoFileClip(video_path)\n",
        "        duration = int(video.duration)\n",
        "        base_name = os.path.splitext(os.path.basename(video_path))[0]\n",
        "\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "        clip_count = 0\n",
        "        for start in range(0, duration, clip_length):\n",
        "            end = min(start + clip_length, duration)\n",
        "            subclip = video.subclip(start, end)\n",
        "            output_path = os.path.join(output_dir, f\"{base_name}_part{clip_count + 1}.mp4\")\n",
        "            subclip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", logger=None)\n",
        "            clip_count += 1\n",
        "\n",
        "        print(f\"‚úÖ Done: {video_path} ‚Üí {clip_count} clips.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Failed to process {video_path}: {e}\")\n",
        "\n",
        "# Process all videos in both folders\n",
        "for label in ['ballet', 'hiphop']:\n",
        "    input_dir = f\"videos/{label}\"\n",
        "    output_dir = f\"clips/{label}\"\n",
        "    for filename in os.listdir(input_dir):\n",
        "        if filename.endswith(\".mp4\"):\n",
        "            full_path = os.path.join(input_dir, filename)\n",
        "            split_video(full_path, clip_length=30, output_dir=output_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eLqkRLn2sGR",
        "outputId": "2ccfc7be-997a-4eae-d3f3-2839b680a285"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Done: videos/ballet/ballet_12.mp4 ‚Üí 1 clips.\n",
            "‚úÖ Done: videos/ballet/ballet_2.mp4 ‚Üí 6 clips.\n",
            "‚úÖ Done: videos/ballet/ballet_1.mp4 ‚Üí 1 clips.\n",
            "‚úÖ Done: videos/ballet/ballet_8.mp4 ‚Üí 1 clips.\n",
            "‚úÖ Done: videos/ballet/ballet_3.mp4 ‚Üí 4 clips.\n",
            "‚úÖ Done: videos/ballet/ballet_13.mp4 ‚Üí 4 clips.\n",
            "‚úÖ Done: videos/ballet/ballet_11.mp4 ‚Üí 2 clips.\n",
            "‚úÖ Done: videos/ballet/ballet_10.mp4 ‚Üí 3 clips.\n",
            "‚úÖ Done: videos/ballet/ballet_5.mp4 ‚Üí 4 clips.\n",
            "‚úÖ Done: videos/ballet/ballet_6.mp4 ‚Üí 2 clips.\n",
            "‚úÖ Done: videos/ballet/ballet_9.mp4 ‚Üí 4 clips.\n",
            "‚úÖ Done: videos/ballet/ballet_7.mp4 ‚Üí 1 clips.\n",
            "‚úÖ Done: videos/ballet/ballet_4.mp4 ‚Üí 1 clips.\n",
            "‚úÖ Done: videos/hiphop/hiphop_9.mp4 ‚Üí 7 clips.\n",
            "‚úÖ Done: videos/hiphop/hiphop_10.mp4 ‚Üí 9 clips.\n",
            "‚úÖ Done: videos/hiphop/hiphop_13.mp4 ‚Üí 8 clips.\n",
            "‚úÖ Done: videos/hiphop/hiphop_6.mp4 ‚Üí 7 clips.\n",
            "‚úÖ Done: videos/hiphop/hiphop_3.mp4 ‚Üí 7 clips.\n",
            "‚úÖ Done: videos/hiphop/hiphop_12.mp4 ‚Üí 5 clips.\n",
            "‚úÖ Done: videos/hiphop/hiphop_11.mp4 ‚Üí 1 clips.\n",
            "‚úÖ Done: videos/hiphop/hiphop_8.mp4 ‚Üí 9 clips.\n",
            "‚úÖ Done: videos/hiphop/hiphop_7.mp4 ‚Üí 5 clips.\n",
            "‚úÖ Done: videos/hiphop/hiphop_5.mp4 ‚Üí 10 clips.\n",
            "‚úÖ Done: videos/hiphop/hiphop_15.mp4 ‚Üí 4 clips.\n",
            "‚úÖ Done: videos/hiphop/hiphop_4.mp4 ‚Üí 1 clips.\n",
            "‚úÖ Done: videos/hiphop/hiphop_1.mp4 ‚Üí 5 clips.\n",
            "‚úÖ Done: videos/hiphop/hiphop_2.mp4 ‚Üí 10 clips.\n",
            "‚úÖ Done: videos/hiphop/hiphop_14.mp4 ‚Üí 6 clips.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import mediapipe as mp\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "metadata": {
        "id": "SmzkCR1Tybtv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Config\n",
        "# ----------------------\n",
        "TARGET_SIZE = (224, 224)\n",
        "SEQUENCE_LENGTH = 32\n",
        "STRIDE = 16\n",
        "USE_SKELETON = True\n",
        "NUM_CLASSES = 5"
      ],
      "metadata": {
        "id": "EJXUdBVRygA8"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Pose Estimation Setup\n",
        "# ----------------------\n",
        "mp_pose = mp.solutions.pose\n",
        "pose_model = mp_pose.Pose(static_image_mode=False, min_detection_confidence=0.5)\n",
        "\n",
        "def extract_pose(frame):\n",
        "    results = pose_model.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "    if not results.pose_landmarks:\n",
        "        return np.zeros((33, 3))  # x, y, z\n",
        "    return np.array([[l.x, l.y, l.z] for l in results.pose_landmarks.landmark])"
      ],
      "metadata": {
        "id": "3EmkRabcyiLW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_video(video_path, use_pose=True, frame_skip=3):\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    frames, keypoints = [], []\n",
        "    frame_idx = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_idx % frame_skip == 0:\n",
        "            frame = cv2.resize(frame, TARGET_SIZE)\n",
        "\n",
        "            if use_pose:\n",
        "                keypoints.append(extract_pose(frame).flatten())\n",
        "\n",
        "            frame = frame.astype(np.float32) / 255.0\n",
        "            frames.append(frame)\n",
        "\n",
        "        frame_idx += 1\n",
        "\n",
        "    cap.release()\n",
        "    return np.array(frames), np.array(keypoints) if use_pose else None"
      ],
      "metadata": {
        "id": "XGxWvYIzyj7u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Create Fixed-Length Clips\n",
        "# ----------------------\n",
        "def create_clips(frames, keypoints=None, sequence_length=32, stride=16):\n",
        "    clips, pose_clips = [], []\n",
        "    for i in range(0, len(frames) - sequence_length + 1, stride):\n",
        "        if keypoints is not None:\n",
        "            pose_clip = keypoints[i:i + sequence_length]\n",
        "            if len(pose_clip) == sequence_length:\n",
        "                pose_clips.append(pose_clip)\n",
        "\n",
        "    return np.array(pose_clips)"
      ],
      "metadata": {
        "id": "v3uIu9WkymBN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PoseTransformer(nn.Module):\n",
        "    def __init__(self, input_size=99, seq_len=32, num_classes=NUM_CLASSES, d_model=128, nhead=4, num_layers=2):\n",
        "        super(PoseTransformer, self).__init__()\n",
        "        self.input_fc = nn.Linear(input_size, d_model)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, d_model))  # learnable [CLS] token\n",
        "        self.fc_out = nn.Linear(d_model, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, _ = x.shape\n",
        "        x = self.input_fc(x)  # [B, T, D]\n",
        "        cls_tokens = self.cls_token.repeat(B, 1, 1)  # [B, 1, D]\n",
        "        x = torch.cat([cls_tokens, x], dim=1)  # [B, T+1, D]\n",
        "        x = x.permute(1, 0, 2)  # Transformer expects [T, B, D]\n",
        "        out = self.transformer(x)[0]  # take the [CLS] token's output\n",
        "        return self.fc_out(out)"
      ],
      "metadata": {
        "id": "Yp8i1yYPyn7f"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ----------------------\n",
        "# Train Model\n",
        "# ----------------------\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def train_model(video_paths, labels):\n",
        "    all_clips = []\n",
        "    all_labels = []\n",
        "\n",
        "    for i, video in enumerate(video_paths):\n",
        "        _, keypoints = process_video(video, use_pose=True, frame_skip=3)\n",
        "        pose_clips = create_clips(_, keypoints, sequence_length=SEQUENCE_LENGTH, stride=STRIDE)\n",
        "        all_clips.extend(pose_clips)\n",
        "        all_labels.extend([labels[i]] * len(pose_clips))\n",
        "\n",
        "    X = np.stack(all_clips)  # shape: [N_clips, 32, 99]\n",
        "    y = np.array(all_labels)\n",
        "\n",
        "    # Split into train/val sets\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Convert to tensors\n",
        "    X_train, y_train = torch.tensor(X_train).float(), torch.tensor(y_train).long()\n",
        "    X_val, y_val = torch.tensor(X_val).float(), torch.tensor(y_val).long()\n",
        "\n",
        "    # Create loaders\n",
        "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=16, shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=16)\n",
        "\n",
        "    # Initialize Transformer model\n",
        "    model = PoseTransformer(input_size=99, seq_len=SEQUENCE_LENGTH, num_classes=NUM_CLASSES,\n",
        "                            d_model=128, nhead=4, num_layers=2)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "    for epoch in range(20):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for xb, yb in train_loader:\n",
        "            pred = model(xb)\n",
        "            loss = criterion(pred, yb)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        with torch.no_grad():\n",
        "            for xb, yb in val_loader:\n",
        "                pred = model(xb)\n",
        "                loss = criterion(pred, yb)\n",
        "                val_loss += loss.item()\n",
        "                correct += (pred.argmax(1) == yb).sum().item()\n",
        "                total += yb.size(0)\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "        print(f\"Epoch {epoch+1} | Train Loss: {running_loss/len(train_loader):.4f} | \"\n",
        "              f\"Val Loss: {val_loss/len(val_loader):.4f} | Val Acc: {100*correct/total:.2f}%\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "sn1qFNhWypto"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ----------------------\n",
        "# Predict on New Video\n",
        "# ----------------------\n",
        "def predict(video_path, model):\n",
        "    _, keypoints = process_video(video_path, use_pose=True)\n",
        "    pose_clips = create_clips(_, keypoints, sequence_length=SEQUENCE_LENGTH, stride=STRIDE)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        inputs = torch.tensor(pose_clips).float()\n",
        "        outputs = model(inputs)\n",
        "        avg_probs = torch.softmax(outputs, dim=1).mean(dim=0)\n",
        "        pred_class = torch.argmax(avg_probs).item()\n",
        "\n",
        "    return pred_class"
      ],
      "metadata": {
        "id": "E2HOajqHyrcA"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def get_labeled_video_paths(clip_dir='clips'):\n",
        "    train_videos, train_labels = [], []\n",
        "    test_videos, test_labels = [], []\n",
        "\n",
        "    for label_name, label_value in [('ballet', 0), ('hiphop', 1)]:\n",
        "        folder_path = os.path.join(clip_dir, label_name)\n",
        "        video_files = sorted([f for f in os.listdir(folder_path) if f.endswith('.mp4')])\n",
        "\n",
        "        if len(video_files) < 2:\n",
        "            raise ValueError(f\"Not enough videos in {folder_path} to split into train/test.\")\n",
        "\n",
        "        random.shuffle(video_files)  # Shuffle to randomize test selection\n",
        "\n",
        "        test_file = video_files.pop()  # Leave one for testing\n",
        "        test_videos.append(os.path.join(folder_path, test_file))\n",
        "        test_labels.append(label_value)\n",
        "\n",
        "        for file in video_files:\n",
        "            train_videos.append(os.path.join(folder_path, file))\n",
        "            train_labels.append(label_value)\n",
        "\n",
        "    return train_videos, train_labels, test_videos, test_labels\n"
      ],
      "metadata": {
        "id": "i5WQbEB76X0r"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "train_videos, train_labels, test_videos, test_labels = get_labeled_video_paths()\n",
        "model = train_model(train_videos, train_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALGJ9kIZyvXu",
        "outputId": "78e6b603-b3e4-4192-d096-4f20312c3935"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:286: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 | Train Loss: 0.5726 | Val Loss: 0.6206 | Val Acc: 59.17%\n",
            "Epoch 2 | Train Loss: 0.5268 | Val Loss: 0.4758 | Val Acc: 76.94%\n",
            "Epoch 3 | Train Loss: 0.5015 | Val Loss: 0.5128 | Val Acc: 76.94%\n",
            "Epoch 4 | Train Loss: 0.4742 | Val Loss: 0.4654 | Val Acc: 81.11%\n",
            "Epoch 5 | Train Loss: 0.4759 | Val Loss: 0.4602 | Val Acc: 84.17%\n",
            "Epoch 6 | Train Loss: 0.4203 | Val Loss: 0.3852 | Val Acc: 84.44%\n",
            "Epoch 7 | Train Loss: 0.4253 | Val Loss: 0.3813 | Val Acc: 84.17%\n",
            "Epoch 8 | Train Loss: 0.4017 | Val Loss: 0.3623 | Val Acc: 85.83%\n",
            "Epoch 9 | Train Loss: 0.3955 | Val Loss: 0.3753 | Val Acc: 86.94%\n",
            "Epoch 10 | Train Loss: 0.3897 | Val Loss: 0.3678 | Val Acc: 85.83%\n",
            "Epoch 11 | Train Loss: 0.3852 | Val Loss: 0.3580 | Val Acc: 86.39%\n",
            "Epoch 12 | Train Loss: 0.3835 | Val Loss: 0.3505 | Val Acc: 86.67%\n",
            "Epoch 13 | Train Loss: 0.3703 | Val Loss: 0.3237 | Val Acc: 87.50%\n",
            "Epoch 14 | Train Loss: 0.3698 | Val Loss: 0.3426 | Val Acc: 87.22%\n",
            "Epoch 15 | Train Loss: 0.3701 | Val Loss: 0.3226 | Val Acc: 88.06%\n",
            "Epoch 16 | Train Loss: 0.3677 | Val Loss: 0.3192 | Val Acc: 88.06%\n",
            "Epoch 17 | Train Loss: 0.3394 | Val Loss: 0.3021 | Val Acc: 88.61%\n",
            "Epoch 18 | Train Loss: 0.3301 | Val Loss: 0.3158 | Val Acc: 88.61%\n",
            "Epoch 19 | Train Loss: 0.3341 | Val Loss: 0.3090 | Val Acc: 90.00%\n",
            "Epoch 20 | Train Loss: 0.3301 | Val Loss: 0.3305 | Val Acc: 88.33%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nüß™ Test Results:\")\n",
        "for i, video_path in enumerate(test_videos):\n",
        "    pred = predict(video_path, model)\n",
        "    print(f\"Video: {os.path.basename(video_path)} | Actual: {test_labels[i]} | Predicted: {pred}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Hul6VNn0WQh",
        "outputId": "50c743e1-5de7-42a1-a135-29ef39e18d2e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üß™ Test Results:\n",
            "Video: ballet_13_part1.mp4 | Actual: 0 | Predicted: 0\n",
            "Video: hiphop_8_part5.mp4 | Actual: 1 | Predicted: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fXM5RcTc3zd4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}